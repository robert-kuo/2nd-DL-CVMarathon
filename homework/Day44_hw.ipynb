{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 讀取資料集以及做前處理的函數\n",
    "def load_data(dirname):\n",
    "    # 讀取 csv 文件\n",
    "    data = pd.read_csv(dirname)\n",
    "    # 過濾有缺失值的 row\n",
    "    data = data.dropna()\n",
    "\n",
    "    # 將圖片像素值讀取為 numpy array 的形態\n",
    "    data['Image'] = data['Image'].apply(lambda img: np.fromstring(img, sep=' ')).values \n",
    "\n",
    "    # 單獨把圖像 array 抽取出來\n",
    "    imgs = np.vstack(data['Image'].values)/255\n",
    "    # reshape 為 96 x 96\n",
    "    imgs = imgs.reshape(data.shape[0], 96, 96)\n",
    "    # 轉換為 float\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    \n",
    "    # 提取坐標的部分\n",
    "    points = data[data.columns[:-1]].values\n",
    "\n",
    "    # 轉換為 float\n",
    "    points = points.astype(np.float32)\n",
    "\n",
    "    # normalize 坐標值到 [-0.5, 0.5]\n",
    "    points = points/96 - 0.5\n",
    "    \n",
    "    return imgs, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "圖像資料: (2140, 96, 96) \n",
      "關鍵點資料: (2140, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "imgs_train, points_train = load_data(dirname = 'training.csv')\n",
    "print(\"圖像資料:\", imgs_train.shape, \"\\n關鍵點資料:\", points_train.shape)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# 回傳定義好的 model 的函數\n",
    "def get_model():\n",
    "    # 定義人臉關鍵點檢測網路\n",
    "    model = Sequential()\n",
    "\n",
    "    # 定義神經網路的輸入\n",
    "    model.add(Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=(96, 96, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 最後輸出 30 維的向量，也就是 15 個關鍵點的值\n",
    "    model.add(Dense(30))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 94, 94, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 45, 45, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 1,424,286\n",
      "Trainable params: 1,424,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# 配置 loss funtion 和 optimizer\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# 印出網路結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1712 samples, validate on 428 samples\n",
      "Epoch 1/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 0.0060 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00209, saving model to best_weights.h5\n",
      "Epoch 2/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00209 to 0.00193, saving model to best_weights.h5\n",
      "Epoch 3/150\n",
      "1712/1712 [==============================] - 28s 16ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00193 to 0.00186, saving model to best_weights.h5\n",
      "Epoch 4/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00186\n",
      "Epoch 5/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00186 to 0.00176, saving model to best_weights.h5\n",
      "Epoch 6/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 9.9537e-04 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00176\n",
      "Epoch 7/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 9.0632e-04 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00176 to 0.00171, saving model to best_weights.h5\n",
      "Epoch 8/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 7.8791e-04 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00171 to 0.00157, saving model to best_weights.h5\n",
      "Epoch 9/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 6.7750e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00157 to 0.00147, saving model to best_weights.h5\n",
      "Epoch 10/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 6.0559e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00147 to 0.00127, saving model to best_weights.h5\n",
      "Epoch 11/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 5.4762e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00127 to 0.00117, saving model to best_weights.h5\n",
      "Epoch 12/150\n",
      "1712/1712 [==============================] - 24s 14ms/step - loss: 4.8969e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00117 to 0.00110, saving model to best_weights.h5\n",
      "Epoch 13/150\n",
      "1712/1712 [==============================] - 24s 14ms/step - loss: 4.4979e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00110 to 0.00108, saving model to best_weights.h5\n",
      "Epoch 14/150\n",
      "1712/1712 [==============================] - 26s 15ms/step - loss: 4.3244e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00108 to 0.00103, saving model to best_weights.h5\n",
      "Epoch 15/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 4.0809e-04 - val_loss: 9.6861e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00103 to 0.00097, saving model to best_weights.h5\n",
      "Epoch 16/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 3.9067e-04 - val_loss: 9.8749e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00097\n",
      "Epoch 17/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 3.7141e-04 - val_loss: 9.3833e-04\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00097 to 0.00094, saving model to best_weights.h5\n",
      "Epoch 18/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 3.5474e-04 - val_loss: 9.2819e-04\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00094 to 0.00093, saving model to best_weights.h5\n",
      "Epoch 19/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 3.3673e-04 - val_loss: 9.0038e-04\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00093 to 0.00090, saving model to best_weights.h5\n",
      "Epoch 20/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 3.3280e-04 - val_loss: 9.1544e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00090\n",
      "Epoch 21/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 3.0828e-04 - val_loss: 8.9882e-04\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00090 to 0.00090, saving model to best_weights.h5\n",
      "Epoch 22/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 3.0563e-04 - val_loss: 8.6997e-04\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00090 to 0.00087, saving model to best_weights.h5\n",
      "Epoch 23/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 2.9840e-04 - val_loss: 8.6209e-04\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00087 to 0.00086, saving model to best_weights.h5\n",
      "Epoch 24/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 2.7778e-04 - val_loss: 8.5313e-04\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00086 to 0.00085, saving model to best_weights.h5\n",
      "Epoch 25/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 2.8281e-04 - val_loss: 8.4837e-04\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00085 to 0.00085, saving model to best_weights.h5\n",
      "Epoch 26/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 2.7147e-04 - val_loss: 8.8775e-04\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00085\n",
      "Epoch 27/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 2.7086e-04 - val_loss: 8.3046e-04\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00085 to 0.00083, saving model to best_weights.h5\n",
      "Epoch 28/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 2.6057e-04 - val_loss: 8.4266e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00083\n",
      "Epoch 29/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 2.4971e-04 - val_loss: 8.7321e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00083\n",
      "Epoch 30/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 2.4302e-04 - val_loss: 8.2128e-04\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00083 to 0.00082, saving model to best_weights.h5\n",
      "Epoch 31/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 2.3712e-04 - val_loss: 8.2142e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00082\n",
      "Epoch 32/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 2.3310e-04 - val_loss: 7.9909e-04\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00082 to 0.00080, saving model to best_weights.h5\n",
      "Epoch 33/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 2.3079e-04 - val_loss: 8.0946e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00080\n",
      "Epoch 34/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 2.2445e-04 - val_loss: 7.9850e-04\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00080 to 0.00080, saving model to best_weights.h5\n",
      "Epoch 35/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 2.1794e-04 - val_loss: 8.2010e-04\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00080\n",
      "Epoch 36/150\n",
      "1712/1712 [==============================] - 20s 11ms/step - loss: 2.1851e-04 - val_loss: 8.1473e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00080\n",
      "Epoch 37/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 2.1485e-04 - val_loss: 8.1966e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00080\n",
      "Epoch 38/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 2.1392e-04 - val_loss: 8.4347e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00080\n",
      "Epoch 39/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 2.1277e-04 - val_loss: 8.3576e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00080\n",
      "Epoch 40/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 2.1146e-04 - val_loss: 8.1325e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00080\n",
      "Epoch 41/150\n",
      "1712/1712 [==============================] - 21s 12ms/step - loss: 2.0424e-04 - val_loss: 7.9287e-04\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00080 to 0.00079, saving model to best_weights.h5\n",
      "Epoch 42/150\n",
      "1712/1712 [==============================] - 24s 14ms/step - loss: 2.0161e-04 - val_loss: 7.9411e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00079\n",
      "Epoch 43/150\n",
      "1712/1712 [==============================] - 23s 13ms/step - loss: 1.9588e-04 - val_loss: 8.1031e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00079\n",
      "Epoch 44/150\n",
      "1712/1712 [==============================] - 21s 12ms/step - loss: 1.9847e-04 - val_loss: 8.0471e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00079\n",
      "Epoch 45/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.9242e-04 - val_loss: 7.9968e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00079\n",
      "Epoch 46/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.8499e-04 - val_loss: 7.9918e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00079\n",
      "Epoch 47/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.8783e-04 - val_loss: 8.1761e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00079\n",
      "Epoch 48/150\n",
      "1712/1712 [==============================] - 24s 14ms/step - loss: 1.8627e-04 - val_loss: 8.0151e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00079\n",
      "Epoch 49/150\n",
      "1712/1712 [==============================] - 24s 14ms/step - loss: 1.8299e-04 - val_loss: 8.2720e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00079\n",
      "Epoch 50/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.7945e-04 - val_loss: 8.3483e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00079\n",
      "Epoch 51/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.7579e-04 - val_loss: 7.7424e-04\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00079 to 0.00077, saving model to best_weights.h5\n",
      "Epoch 52/150\n",
      "1712/1712 [==============================] - 25s 15ms/step - loss: 1.6936e-04 - val_loss: 7.8136e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00077\n",
      "Epoch 53/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.7450e-04 - val_loss: 8.0681e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00077\n",
      "Epoch 54/150\n",
      "1712/1712 [==============================] - 21s 12ms/step - loss: 1.7319e-04 - val_loss: 8.8349e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00077\n",
      "Epoch 55/150\n",
      "1712/1712 [==============================] - 20s 11ms/step - loss: 1.7176e-04 - val_loss: 8.0494e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00077\n",
      "Epoch 56/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.6856e-04 - val_loss: 8.0610e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00077\n",
      "Epoch 57/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.6113e-04 - val_loss: 7.9077e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00077\n",
      "Epoch 58/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.6291e-04 - val_loss: 8.4044e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00077\n",
      "Epoch 59/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.6074e-04 - val_loss: 7.8800e-04\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00077\n",
      "Epoch 60/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.6209e-04 - val_loss: 7.9221e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00077\n",
      "Epoch 61/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.5604e-04 - val_loss: 7.7270e-04\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00077 to 0.00077, saving model to best_weights.h5\n",
      "Epoch 62/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.5463e-04 - val_loss: 8.1207e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00077\n",
      "Epoch 63/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.5632e-04 - val_loss: 8.6114e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00077\n",
      "Epoch 64/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.5682e-04 - val_loss: 8.1614e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00077\n",
      "Epoch 65/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.5855e-04 - val_loss: 7.9366e-04\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00077\n",
      "Epoch 66/150\n",
      "1712/1712 [==============================] - 20s 11ms/step - loss: 1.5367e-04 - val_loss: 8.2621e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00077\n",
      "Epoch 67/150\n",
      "1712/1712 [==============================] - 26s 15ms/step - loss: 1.4383e-04 - val_loss: 7.7415e-04\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00077\n",
      "Epoch 68/150\n",
      "1712/1712 [==============================] - 24s 14ms/step - loss: 1.4354e-04 - val_loss: 8.1548e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00077\n",
      "Epoch 69/150\n",
      "1712/1712 [==============================] - 30s 17ms/step - loss: 1.4303e-04 - val_loss: 7.9433e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00077\n",
      "Epoch 70/150\n",
      "1712/1712 [==============================] - 31s 18ms/step - loss: 1.4195e-04 - val_loss: 7.9656e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00077\n",
      "Epoch 71/150\n",
      "1712/1712 [==============================] - 26s 15ms/step - loss: 1.4496e-04 - val_loss: 8.1751e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00077\n",
      "Epoch 72/150\n",
      "1712/1712 [==============================] - 27s 16ms/step - loss: 1.4658e-04 - val_loss: 7.9281e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00077\n",
      "Epoch 73/150\n",
      "1712/1712 [==============================] - 28s 16ms/step - loss: 1.3763e-04 - val_loss: 7.9921e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00077\n",
      "Epoch 74/150\n",
      "1712/1712 [==============================] - 21s 12ms/step - loss: 1.3685e-04 - val_loss: 7.9446e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00077\n",
      "Epoch 75/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.3647e-04 - val_loss: 7.9449e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00077\n",
      "Epoch 76/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.3856e-04 - val_loss: 7.7412e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00077\n",
      "Epoch 77/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.3590e-04 - val_loss: 7.7694e-04\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00077\n",
      "Epoch 78/150\n",
      "1712/1712 [==============================] - 22s 13ms/step - loss: 1.3207e-04 - val_loss: 8.0658e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00077\n",
      "Epoch 79/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.2854e-04 - val_loss: 7.8509e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00077\n",
      "Epoch 80/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.3094e-04 - val_loss: 7.8222e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00077\n",
      "Epoch 81/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.3113e-04 - val_loss: 7.8557e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00077\n",
      "Epoch 82/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.2820e-04 - val_loss: 7.7928e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00077\n",
      "Epoch 83/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.3063e-04 - val_loss: 8.1483e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00077\n",
      "Epoch 84/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.2879e-04 - val_loss: 7.9904e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00077\n",
      "Epoch 85/150\n",
      "1712/1712 [==============================] - 18s 10ms/step - loss: 1.2475e-04 - val_loss: 7.6599e-04\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00077 to 0.00077, saving model to best_weights.h5\n",
      "Epoch 86/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.2416e-04 - val_loss: 7.7023e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00077\n",
      "Epoch 87/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.2300e-04 - val_loss: 7.6234e-04\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00077 to 0.00076, saving model to best_weights.h5\n",
      "Epoch 88/150\n",
      "1712/1712 [==============================] - 22s 13ms/step - loss: 1.2203e-04 - val_loss: 8.3979e-04\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00076\n",
      "Epoch 89/150\n",
      "1712/1712 [==============================] - 25s 14ms/step - loss: 1.2278e-04 - val_loss: 7.7355e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00076\n",
      "Epoch 90/150\n",
      "1712/1712 [==============================] - 26s 15ms/step - loss: 1.2311e-04 - val_loss: 7.8101e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00076\n",
      "Epoch 91/150\n",
      "1712/1712 [==============================] - 20s 11ms/step - loss: 1.2001e-04 - val_loss: 8.1039e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00076\n",
      "Epoch 92/150\n",
      "1712/1712 [==============================] - 29s 17ms/step - loss: 1.1940e-04 - val_loss: 7.9389e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00076\n",
      "Epoch 93/150\n",
      "1712/1712 [==============================] - 24s 14ms/step - loss: 1.1835e-04 - val_loss: 8.0572e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00076\n",
      "Epoch 94/150\n",
      "1712/1712 [==============================] - 21s 12ms/step - loss: 1.1701e-04 - val_loss: 7.9057e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00076\n",
      "Epoch 95/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.1742e-04 - val_loss: 7.6218e-04\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00076 to 0.00076, saving model to best_weights.h5\n",
      "Epoch 96/150\n",
      "1712/1712 [==============================] - 21s 12ms/step - loss: 1.1745e-04 - val_loss: 7.6775e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00076\n",
      "Epoch 97/150\n",
      "1712/1712 [==============================] - 22s 13ms/step - loss: 1.1208e-04 - val_loss: 7.5517e-04\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00076 to 0.00076, saving model to best_weights.h5\n",
      "Epoch 98/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.1493e-04 - val_loss: 7.9669e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00076\n",
      "Epoch 99/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.1404e-04 - val_loss: 7.9385e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00076\n",
      "Epoch 100/150\n",
      "1712/1712 [==============================] - 20s 11ms/step - loss: 1.1463e-04 - val_loss: 8.0888e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00076\n",
      "Epoch 101/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.0953e-04 - val_loss: 8.0127e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00076\n",
      "Epoch 102/150\n",
      "1712/1712 [==============================] - 21s 12ms/step - loss: 1.0857e-04 - val_loss: 7.8689e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00076\n",
      "Epoch 103/150\n",
      "1712/1712 [==============================] - 25s 15ms/step - loss: 1.1163e-04 - val_loss: 7.9109e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00076\n",
      "Epoch 104/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.1179e-04 - val_loss: 7.8371e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00076\n",
      "Epoch 105/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.1108e-04 - val_loss: 8.0291e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00076\n",
      "Epoch 106/150\n",
      "1712/1712 [==============================] - 20s 12ms/step - loss: 1.0814e-04 - val_loss: 7.8621e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00076\n",
      "Epoch 107/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.0997e-04 - val_loss: 8.5866e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00076\n",
      "Epoch 108/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.0974e-04 - val_loss: 7.9529e-04\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00076\n",
      "Epoch 109/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.0742e-04 - val_loss: 8.3568e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00076\n",
      "Epoch 110/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.0308e-04 - val_loss: 7.8434e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00076\n",
      "Epoch 111/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.0239e-04 - val_loss: 7.7432e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00076\n",
      "Epoch 112/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.0651e-04 - val_loss: 7.9926e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00076\n",
      "Epoch 113/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.0292e-04 - val_loss: 7.8257e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00076\n",
      "Epoch 114/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.0391e-04 - val_loss: 7.9269e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00076\n",
      "Epoch 115/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.0102e-04 - val_loss: 7.8492e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00076\n",
      "Epoch 116/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.0148e-04 - val_loss: 8.0869e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00076\n",
      "Epoch 117/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 1.0257e-04 - val_loss: 7.7747e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00076\n",
      "Epoch 118/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.9273e-05 - val_loss: 8.1619e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00076\n",
      "Epoch 119/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 1.0293e-04 - val_loss: 8.2445e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00076\n",
      "Epoch 120/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.9855e-05 - val_loss: 7.9804e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00076\n",
      "Epoch 121/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.5713e-05 - val_loss: 8.2382e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00076\n",
      "Epoch 122/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.5196e-05 - val_loss: 8.0532e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00076\n",
      "Epoch 123/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 9.7439e-05 - val_loss: 7.9986e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00076\n",
      "Epoch 124/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.5646e-05 - val_loss: 7.8499e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00076\n",
      "Epoch 125/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 9.8087e-05 - val_loss: 7.8419e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00076\n",
      "Epoch 126/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 9.7303e-05 - val_loss: 7.9379e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00076\n",
      "Epoch 127/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.5462e-05 - val_loss: 7.9361e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00076\n",
      "Epoch 128/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.5919e-05 - val_loss: 7.9160e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00076\n",
      "Epoch 129/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.4257e-05 - val_loss: 7.9878e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00076\n",
      "Epoch 130/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 9.3426e-05 - val_loss: 7.8245e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00076\n",
      "Epoch 131/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.2376e-05 - val_loss: 8.0306e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00076\n",
      "Epoch 132/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.2904e-05 - val_loss: 8.6197e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00076\n",
      "Epoch 133/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 9.1842e-05 - val_loss: 7.8789e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00076\n",
      "Epoch 134/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 9.0004e-05 - val_loss: 8.0482e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00076\n",
      "Epoch 135/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.0770e-05 - val_loss: 7.9598e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00076\n",
      "Epoch 136/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.0110e-05 - val_loss: 7.9724e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00076\n",
      "Epoch 137/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 8.8748e-05 - val_loss: 7.6617e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00076\n",
      "Epoch 138/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 8.9623e-05 - val_loss: 7.9318e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00076\n",
      "Epoch 139/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 9.0910e-05 - val_loss: 7.8139e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00076\n",
      "Epoch 140/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 8.8347e-05 - val_loss: 7.7586e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00076\n",
      "Epoch 141/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 8.9114e-05 - val_loss: 8.1521e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00076\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712/1712 [==============================] - 18s 11ms/step - loss: 8.9330e-05 - val_loss: 8.0720e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00076\n",
      "Epoch 143/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 8.8426e-05 - val_loss: 7.8756e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00076\n",
      "Epoch 144/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 8.7255e-05 - val_loss: 7.9708e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00076\n",
      "Epoch 145/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 8.5229e-05 - val_loss: 7.7876e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00076\n",
      "Epoch 146/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 8.6890e-05 - val_loss: 8.1039e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00076\n",
      "Epoch 147/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 8.6220e-05 - val_loss: 7.9642e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00076\n",
      "Epoch 148/150\n",
      "1712/1712 [==============================] - 19s 11ms/step - loss: 8.4736e-05 - val_loss: 8.2017e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00076\n",
      "Epoch 149/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 8.5946e-05 - val_loss: 7.8873e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00076\n",
      "Epoch 150/150\n",
      "1712/1712 [==============================] - 18s 11ms/step - loss: 8.5980e-05 - val_loss: 8.1682e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a7642f9160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, History\n",
    "# model checkpoint \n",
    "checkpoint = ModelCheckpoint('best_weights.h5', verbose=1, save_best_only=True)\n",
    "hist = History()\n",
    "\n",
    "\n",
    "# training the model\n",
    "hist_model = model.fit(imgs_train.reshape(-1, 96, 96, 1), \n",
    "                       points_train, \n",
    "                       validation_split=0.2, batch_size=64, callbacks=[checkpoint, hist],\n",
    "                       shuffle=True, epochs=150, verbose=1)\n",
    "# save the model weights\n",
    "model.save_weights('weights.h5')\n",
    "# save the model\n",
    "model.save('model.h5')\n",
    "\n",
    "\n",
    "# loss 值的圖\n",
    "plt.title('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(hist_model.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(hist_model.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
